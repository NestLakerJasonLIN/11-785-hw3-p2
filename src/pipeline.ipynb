{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import phoneme_list\n",
    "import ctcdecode\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import Levenshtein\n",
    "\n",
    "verbose = True\n",
    "mode = \"development\"\n",
    "cuda = torch.cuda.is_available()\n",
    "num_workers = 4 if cuda else 0 \n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if verbose:\n",
    "    print(\"mode: %s\" % mode)\n",
    "    print(\"torch version: %s\" % torch.__version__)\n",
    "    print(\"np version: %s\" % np.__version__)\n",
    "    print(\"cuda: %s\" % cuda)\n",
    "    print(\"num_workers: %s\" % num_workers)\n",
    "    print(\"device: %s\" % device)\n",
    "    print(\"verbose: %s\" % verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/\"\n",
    "train_path = data_path + \"wsj0_train\"\n",
    "dev_path = data_path + \"wsj0_dev.npy\"\n",
    "test_path = data_path + \"wsj0_test\"\n",
    "train_merged_labels_path = data_path + \"wsj0_train_merged_labels.npy\"\n",
    "dev_merged_labels_path = data_path + \"wsj0_dev_merged_labels.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == 'actual':\n",
    "    train = np.load(train_path, allow_pickle=True)\n",
    "    train_merged_labels = np.load(train_merged_labels_path, allow_pickle=True)\n",
    "else:\n",
    "    train = np.load(dev_path, allow_pickle=True)\n",
    "    train_merged_labels = np.load(dev_merged_labels_path, allow_pickle=True)\n",
    "\n",
    "dev = np.load(dev_path, allow_pickle=True)\n",
    "dev_merged_labels = np.load(dev_merged_labels_path, allow_pickle=True)\n",
    "test = np.load(test_path, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleDataset(Dataset):\n",
    "    def __init__(self, x, y=None, is_test=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.is_test = is_test\n",
    "        self._x = x\n",
    "        self._y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._x)\n",
    "      \n",
    "    def __getitem__(self, index):\n",
    "        if not self.is_test:\n",
    "            return torch.from_numpy(self._x[index]).float(), torch.from_numpy(self._y[index])\n",
    "        else:\n",
    "            return torch.from_numpy(self._x[index]).float()\n",
    "\n",
    "# customize pinned memory for fast host-gpu copies\n",
    "class CustomBatch:\n",
    "    def __init__(self, batch, is_test=False):\n",
    "        # reference: https://suzyahyah.github.io/pytorch/2019/07/01/DataLoader-Pad-Pack-Sequence.html\n",
    "        if not is_test:\n",
    "            data, target = zip(*batch)\n",
    "\n",
    "            self.data_lens = [len(x) for x in data]\n",
    "            self.target_lens = [len(y) for y in target]\n",
    "\n",
    "            self.data = pad_sequence(data, batch_first=True)\n",
    "            self.target = pad_sequence(target, batch_first=True)\n",
    "        else:\n",
    "            data = batch\n",
    "            self.data_lens = [len(x) for x in data]\n",
    "            self.data = pad_sequence(data, batch_first=True)\n",
    "\n",
    "    # custom memory pinning method on custom type\n",
    "    def pin_memory(self):\n",
    "        # TODO: check if this really works\n",
    "        self.data = self.data.pin_memory()\n",
    "        if self.target is not None:\n",
    "            self.target = self.target.pin_memory()\n",
    "        return self\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return CustomBatch(batch)\n",
    "\n",
    "def collate_fn_test(batch):\n",
    "    return CustomBatch(batch, is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = simpleDataset(train, train_merged_labels)\n",
    "dev_dataset = simpleDataset(dev, dev_merged_labels)\n",
    "test_dataset = simpleDataset(test, is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "batch_size = 2\n",
    "input_size = 40\n",
    "hidden_size = 47\n",
    "output_size = 47\n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "bidirectional = True\n",
    "lr = 0.01\n",
    "beam_size = 5\n",
    "blank_idx = 46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "                train_dataset,              # The dataset\n",
    "                batch_size=batch_size,      # Batch size\n",
    "                shuffle=True,               # Shuffles the dataset at every epoch\n",
    "                pin_memory=True,            # Copy data to CUDA pinned memory\n",
    "                num_workers=num_workers,    # Number of worker processes for loading data.\n",
    "                collate_fn = collate_fn\n",
    "               )\n",
    "\n",
    "dev_loader = DataLoader(\n",
    "                dev_dataset,              # The dataset\n",
    "                batch_size=batch_size,      # Batch size\n",
    "                shuffle=False,               # Shuffles the dataset at every epoch\n",
    "                pin_memory=True,            # Copy data to CUDA pinned memory\n",
    "                num_workers=num_workers,    # Number of worker processes for loading data.\n",
    "                collate_fn = collate_fn\n",
    "               )\n",
    "\n",
    "test_loader = DataLoader(\n",
    "                test_dataset,              # The dataset\n",
    "                batch_size=batch_size,      # Batch size\n",
    "                shuffle=False,               # Shuffles the dataset at every epoch\n",
    "                pin_memory=True,            # Copy data to CUDA pinned memory\n",
    "                num_workers=num_workers,    # Number of worker processes for loading data.\n",
    "                collate_fn = collate_fn_test\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, bidirectional, dropout):\n",
    "        super(Baseline, self).__init__()\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size=input_size, \n",
    "                          hidden_size=hidden_size,\n",
    "                          num_layers = num_layers,\n",
    "                          batch_first = True,\n",
    "                          dropout = dropout,\n",
    "                          bidirectional = bidirectional\n",
    "                          )\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size*2, output_size)\n",
    "\n",
    "    def forward(self, data, data_lens):\n",
    "        # pack too rnn\n",
    "        data_packed = pack_padded_sequence(data, data_lens, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        output_packed, (hn, cn) = self.rnn(data_packed)\n",
    "        \n",
    "        # unpack from rnn\n",
    "        output_padded, output_lengths = pad_packed_sequence(output_packed, batch_first=True)\n",
    "\n",
    "        # output shape: (batch_size, seq_len, output_size)\n",
    "        output = self.linear(output_padded)\n",
    "\n",
    "        # TODO: softmax considered packed value which should not be considered\n",
    "        # do softmax before pass to CTC loss\n",
    "        output = F.log_softmax(output, dim=-1)\n",
    "\n",
    "        return output, output_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Baseline(input_size, hidden_size, output_size, num_layers, bidirectional, dropout)\n",
    "optimizer = optim.SGD(model.to(device).parameters(), lr=lr, momentum=0.9, weight_decay=5e-4, nesterov=True)   # optimize all cnn parameters\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "criterion = nn.CTCLoss(blank=blank_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "running_loss = 0.0\n",
    "total_predictions = 0.0\n",
    "correct_predictions = 0.0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for batch_idx, sample in enumerate(train_loader):\n",
    "    data, target = sample.data, sample.target\n",
    "    data_lens, target_lens = sample.data_lens, sample.target_lens\n",
    "    assert data.shape[1] == max(data_lens)\n",
    "    assert target.shape[1] == max(target_lens)\n",
    "    \n",
    "    outputs, output_lens = model(data, data_lens)\n",
    "    \n",
    "    loss = criterion(log_probs = outputs.permute(1, 0, 2), \n",
    "          targets = target, \n",
    "          input_lengths = output_lengths, \n",
    "          target_lengths = torch.tensor(target_lens))\n",
    "  \n",
    "    running_loss += loss.item()\n",
    "\n",
    "    accuracy = get_accuracy(outputs, target, output_lens, target_lens)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    break\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "running_loss /= len(train_loader)\n",
    "# acc = (correct_predictions / total_predictions) * 100.0\n",
    "print('Training Loss: ', running_loss, 'Time: ',end_time - start_time, 's')\n",
    "# print('Training Accuracy: ', acc, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(outputs, target, output_lens, target_lens):\n",
    "    # outputs: log_softmax output from model\n",
    "    \n",
    "    # TODO: step1: decode outputs using CTC beamsearch\n",
    "    \n",
    "    # step2: calculate Levenshtein distance as accuracy\n",
    "    Levenshtein.distance(preds, golds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: batch process\n",
    "def convert_to_string(tokens, vocab, seq_len):\n",
    "    return ''.join([vocab[x] for x in tokens[0:seq_len]])\n",
    "\n",
    "# TODO: use what to represent blank symbol ?\n",
    "vocab_list = phoneme_list.PHONEME_MAP + ['#']\n",
    "# TODO: use '_' as stop sign?\n",
    "decoder = ctcdecode.CTCBeamDecoder(labels=vocab_list, \n",
    "                                   beam_width=beam_size,\n",
    "                                   blank_id=blank_idx,\n",
    "                                   log_probs_input=True)\n",
    "beam_result, beam_scores, timesteps, out_seq_len = decoder.decode(outputs, seq_lens=output_lens)\n",
    "\n",
    "preds = convert_to_string(beam_result[0][0], vocab_list, out_seq_len[0][0])\n",
    "golds = convert_to_string(target[0], vocab_list, target_lens[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
